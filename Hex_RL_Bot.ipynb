{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab notebook (some edits were made to the class to accomodate environment)"
      ],
      "metadata": {
        "id": "hcXybCWMouwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scroll to way bottom of the notebook for latest model training and tournament play\n",
        "\n",
        "## Original Preliminary Setup (Obsolete):"
      ],
      "metadata": {
        "id": "NOC6vxlR7uUH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r68RZv2LktwC"
      },
      "outputs": [],
      "source": [
        "# !pip install stable-baselines3[extra] gym torch --quiet\n",
        "# !pip install shimmy\n",
        "#!pip install gym==0.26.2 gymnasium shimmy --upgrade\n",
        "\n",
        "# Don't rely on these to install, had to go through a bunch of edge cases\n",
        "#   so you won't need some of these"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class HexEnv(gym.Env):\n",
        "    def __init__(self, size=5):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.action_space = spaces.Discrete(size * size)\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(size * size,), dtype=np.float32)\n",
        "        self.board = None\n",
        "        self.current_player = 1\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.board = np.zeros((self.size, self.size), dtype=np.int8)\n",
        "        self.current_player = 1\n",
        "        return self.board.flatten().astype(np.float32), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = divmod(action, self.size)\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        if self.board[x][y] != 0:\n",
        "            reward = -10.0\n",
        "            terminated = True\n",
        "        else:\n",
        "            self.board[x][y] = self.current_player\n",
        "            won = self.check_win()\n",
        "            reward = 1.0 if won else 0.0\n",
        "            terminated = won\n",
        "            self.current_player *= -1\n",
        "\n",
        "        return self.board.flatten().astype(np.float32), reward, terminated, truncated, {}\n",
        "\n",
        "    def check_win(self):\n",
        "        size = self.size\n",
        "        player = self.current_player\n",
        "        visited = set()\n",
        "\n",
        "        def dfs(x, y):\n",
        "            if (x, y) in visited:\n",
        "                return False\n",
        "            visited.add((x, y))\n",
        "\n",
        "            if player == 1 and x == size - 1:\n",
        "                return True\n",
        "            if player == -1 and y == size - 1:\n",
        "                return True\n",
        "\n",
        "            directions = [(-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0)]\n",
        "            for dx, dy in directions:\n",
        "                nx, ny = x + dx, y + dy\n",
        "                if 0 <= nx < size and 0 <= ny < size:\n",
        "                    if self.board[nx][ny] == player:\n",
        "                        if dfs(nx, ny):\n",
        "                            return True\n",
        "            return False\n",
        "\n",
        "        if player == 1:\n",
        "            for col in range(size):\n",
        "                if self.board[0][col] == player:\n",
        "                    if dfs(0, col):\n",
        "                        return True\n",
        "        else:\n",
        "            for row in range(size):\n",
        "                if self.board[row][0] == player:\n",
        "                    if dfs(row, 0):\n",
        "                        return True\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "bF_WmZ60kww4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and train\n"
      ],
      "metadata": {
        "id": "0ntPr9APlrxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "env = HexEnv(size=5)\n",
        "check_env(env)\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=100_000)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mvdUXMKQlzIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Testing (ensure working)"
      ],
      "metadata": {
        "id": "lt89sdKHo98s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple move"
      ],
      "metadata": {
        "id": "BF6LPsA8pY9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = HexEnv(size=5)\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# Let the model pick a move\n",
        "action, _ = model.predict(obs)\n",
        "\n",
        "# Convert the flat action index to (x, y)\n",
        "x, y = divmod(action, env.size)\n",
        "print(f\"Model move: ({x}, {y})\")\n",
        "\n",
        "# Show board with the move placed\n",
        "env.board[x][y] = env.current_player\n",
        "print(env.board)\n"
      ],
      "metadata": {
        "id": "M5wgZBjEo_yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play against it"
      ],
      "metadata": {
        "id": "1Sia02j6pdFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = HexEnv(size=5)\n",
        "obs, _ = env.reset()\n",
        "\n",
        "def print_board(b):\n",
        "    symbols = {0: \".\", 1: \"X\", -1: \"O\"}\n",
        "    for row in b:\n",
        "        print(\" \".join(symbols[int(cell)] for cell in row))\n",
        "    print()\n",
        "\n",
        "print(\"You are Player 1 (X), bot is Player -1 (O)\")\n",
        "print_board(env.board)\n"
      ],
      "metadata": {
        "id": "YvbAI4YupHgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Human Player 1 (X) Move ===\n",
        "while True:\n",
        "    try:\n",
        "        x = int(input(\"Enter your row (0-4): \"))\n",
        "        y = int(input(\"Enter your column (0-4): \"))\n",
        "        if env.board[x][y] == 0:\n",
        "            env.board[x][y] = 1\n",
        "            env.current_player = -1  # switch to bot\n",
        "            break\n",
        "        else:\n",
        "            print(\"Cell already taken! Try again.\")\n",
        "    except Exception as e:\n",
        "        print(\"Invalid input. Try again.\")\n",
        "\n",
        "print(\"You moved:\")\n",
        "print_board(env.board)\n"
      ],
      "metadata": {
        "id": "8xXuYNXlpTM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full game testing"
      ],
      "metadata": {
        "id": "gv2xzuaVp4ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = HexEnv(size=5)\n",
        "obs, _ = env.reset()\n",
        "\n",
        "def print_board(b):\n",
        "    symbols = {0: \".\", 1: \"X\", -1: \"O\"}\n",
        "    for row in b:\n",
        "        print(\" \".join(symbols[int(cell)] for cell in row))\n",
        "    print()\n",
        "\n",
        "print(\"Welcome to Hex! You are Player 1 (X, top to bottom).\")\n",
        "print(\"Bot is Player -1 (O, left to right).\")\n",
        "print_board(env.board)\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    # === Human Turn ===\n",
        "    while True:\n",
        "        try:\n",
        "            x = int(input(\"Enter your row (0-4): \"))\n",
        "            y = int(input(\"Enter your column (0-4): \"))\n",
        "            if env.board[x][y] == 0:\n",
        "                env.board[x][y] = 1\n",
        "                env.current_player = 1\n",
        "                if env.check_win():\n",
        "                    print_board(env.board)\n",
        "                    print(\"You win!\")\n",
        "                    done = True\n",
        "                else:\n",
        "                    env.current_player = -1\n",
        "                break\n",
        "            else:\n",
        "                print(\"Cell already taken! Try again.\")\n",
        "        except:\n",
        "            print(\"Invalid input. Try again.\")\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "    print_board(env.board)\n",
        "\n",
        "    # === Bot Turn ===\n",
        "    obs = env.board.flatten().astype(np.float32)\n",
        "    action, _ = model.predict(obs)\n",
        "    bot_x, bot_y = divmod(action, env.size)\n",
        "    print(f\"Bot plays: ({bot_x}, {bot_y})\")\n",
        "\n",
        "    if env.board[bot_x][bot_y] != 0:\n",
        "        print(\"Bot chose an illegal move\")\n",
        "        break\n",
        "\n",
        "    env.board[bot_x][bot_y] = -1\n",
        "    env.current_player = -1\n",
        "    if env.check_win():\n",
        "        print_board(env.board)\n",
        "        print(\"Bot wins\")\n",
        "        done = True\n",
        "    else:\n",
        "        env.current_player = 1\n",
        "\n",
        "    print_board(env.board)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qXTPXXqxpfuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More training (Obsolete, scroll down for self-play training)"
      ],
      "metadata": {
        "id": "rZ9McyEyqZN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Re-initialize just for sanity check\n",
        "#   Note: I'll start randomizing different dimensions next time I look at this\n",
        "env = HexEnv(size=5)\n",
        "\n",
        "# Directory to save models\n",
        "save_dir = \"hex_bot_checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# load/initialize model\n",
        "model_path = f\"{save_dir}/hex_bot_0.zip\"\n",
        "if os.path.exists(model_path):\n",
        "    model = PPO.load(model_path, env=env)\n",
        "    print(\"Loaded model from previous checkpoint.\")\n",
        "else:\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "    print(\"Starting fresh.\")\n",
        "\n",
        "# TRAINING LOOP\n",
        "total_checkpoints = 5\n",
        "timesteps_per_checkpoint = 100_000\n",
        "\n",
        "for i in range(1, total_checkpoints + 1):\n",
        "    print(f\"\\Training: Checkpoint {i} ({i * timesteps_per_checkpoint} steps total)\")\n",
        "    model.learn(total_timesteps=timesteps_per_checkpoint)\n",
        "\n",
        "    # Save\n",
        "    checkpoint_path = f\"{save_dir}/hex_bot_{i * timesteps_per_checkpoint}.zip\"\n",
        "    model.save(checkpoint_path)\n",
        "    print(f\"Saved model to {checkpoint_path}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ya3BxQZXptmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reloading\n",
        "env = HexEnv(size=5)\n",
        "model = PPO.load(\"hex_bot_checkpoints/hex_bot_500000\", env=env)\n",
        "\n",
        "print(\"Loaded bot from checkpoint with 500k steps.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_jsmDmolujqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor, softmax\n",
        "import torch\n",
        "\n",
        "env = HexEnv(size=5)\n",
        "obs, _ = env.reset()\n",
        "\n",
        "def print_board(b):\n",
        "    symbols = {0: \".\", 1: \"X\", -1: \"O\"}\n",
        "    for row in b:\n",
        "        print(\" \".join(symbols[int(cell)] for cell in row))\n",
        "    print()\n",
        "\n",
        "print(\"Welcome to Hex! You are Player 1 (X, top to bottom).\")\n",
        "print(\"Bot is Player -1 (O, left to right).\")\n",
        "print_board(env.board)\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    # === Human Turn ===\n",
        "    while True:\n",
        "        try:\n",
        "            x = int(input(\"Enter your row (0-4): \"))\n",
        "            y = int(input(\"Enter your column (0-4): \"))\n",
        "            if env.board[x][y] == 0:\n",
        "                env.board[x][y] = 1\n",
        "                env.current_player = 1\n",
        "                if env.check_win():\n",
        "                    print_board(env.board)\n",
        "                    print(\"You win!\")\n",
        "                    done = True\n",
        "                else:\n",
        "                    env.current_player = -1\n",
        "                break\n",
        "            else:\n",
        "                print(\"Cell already taken! Try again.\")\n",
        "        except:\n",
        "            print(\"Invalid input. Try again.\")\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "    print_board(env.board)\n",
        "\n",
        "    # === Bot Turn (with legal move filtering cuz it still does illegal stuff) ===\n",
        "    obs = env.board.flatten().astype(np.float32)\n",
        "    obs_tensor = torch.tensor(obs).unsqueeze(0)  # shape: [1, obs_dim]\n",
        "\n",
        "    # Get legal moves\n",
        "    legal_moves = [(i, j) for i in range(env.size) for j in range(env.size) if env.board[i][j] == 0]\n",
        "    legal_indices = [i * env.size + j for i, j in legal_moves]\n",
        "\n",
        "    # Get action probabilities\n",
        "    dist = model.policy.get_distribution(obs_tensor)\n",
        "    action_probs = dist.distribution.probs.detach().numpy().flatten()\n",
        "\n",
        "    # Mask illegal moves\n",
        "    masked_probs = np.zeros_like(action_probs)\n",
        "    masked_probs[legal_indices] = action_probs[legal_indices]\n",
        "    masked_probs /= masked_probs.sum()\n",
        "\n",
        "    # Sample from legal actions\n",
        "    action = np.random.choice(len(masked_probs), p=masked_probs)\n",
        "    bot_x, bot_y = divmod(action, env.size)\n",
        "    print(f\"Bot plays: ({bot_x}, {bot_y})\")\n",
        "\n",
        "    # Apply it\n",
        "    env.board[bot_x][bot_y] = -1\n",
        "    env.current_player = -1\n",
        "    if env.check_win():\n",
        "        print_board(env.board)\n",
        "        print(\"Bot wins!\")\n",
        "        done = True\n",
        "    else:\n",
        "        env.current_player = 1\n",
        "\n",
        "    print_board(env.board)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CidMZWB_qeyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self Play Training - LATEST**"
      ],
      "metadata": {
        "id": "mkGVW1u5xRVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repasting a bunch of stuff as well just for ease of access"
      ],
      "metadata": {
        "id": "1hzUBS9IxY4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class SelfPlayHexEnv(gym.Env):\n",
        "    def __init__(self, size=5):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.action_space = spaces.Discrete(size * size)\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(size * size,), dtype=np.float32)\n",
        "        self.board = None\n",
        "        self.current_player = 1  # alternates during reset\n",
        "        self.agent_player = 1  # will be randomly assigned\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.board = np.zeros((self.size, self.size), dtype=np.int8)\n",
        "        self.agent_player = random.choice([1, -1])\n",
        "        self.current_player = 1  # game always starts with Player 1\n",
        "        self.done = False\n",
        "\n",
        "        # If agent is Player -1, let Player 1 (opponent) make a random move\n",
        "        if self.agent_player == -1:\n",
        "            opp_moves = [(i, j) for i in range(self.size) for j in range(self.size)]\n",
        "            move = random.choice(opp_moves)\n",
        "            self.board[move] = 1\n",
        "\n",
        "        return self.board.flatten().astype(np.float32), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            raise RuntimeError(\"Episode has ended, please reset.\")\n",
        "\n",
        "        x, y = divmod(action, self.size)\n",
        "\n",
        "        # Illegal move\n",
        "        if self.board[x][y] != 0:\n",
        "            self.done = True\n",
        "            return self.board.flatten().astype(np.float32), -10.0, True, False, {}\n",
        "\n",
        "        self.board[x][y] = self.agent_player\n",
        "\n",
        "        # Check if agent won\n",
        "        self.current_player = self.agent_player\n",
        "        if self.check_win():\n",
        "            self.done = True\n",
        "            return self.board.flatten().astype(np.float32), 1.0, True, False, {}\n",
        "\n",
        "        # Opponent random move\n",
        "        opp_player = -self.agent_player\n",
        "        legal = [(i, j) for i in range(self.size) for j in range(self.size) if self.board[i][j] == 0]\n",
        "        if not legal:\n",
        "            self.done = True\n",
        "            return self.board.flatten().astype(np.float32), 0.0, True, False, {}\n",
        "\n",
        "        move = random.choice(legal)\n",
        "        self.board[move] = opp_player\n",
        "\n",
        "        # Check if opponent won\n",
        "        self.current_player = opp_player\n",
        "        if self.check_win():\n",
        "            self.done = True\n",
        "            return self.board.flatten().astype(np.float32), -1.0, True, False, {}\n",
        "\n",
        "        # Game continues\n",
        "        return self.board.flatten().astype(np.float32), 0.0, False, False, {}\n",
        "\n",
        "    def check_win(self):\n",
        "        size = self.size\n",
        "        player = self.current_player\n",
        "        visited = set()\n",
        "\n",
        "        def dfs(x, y):\n",
        "            if (x, y) in visited:\n",
        "                return False\n",
        "            visited.add((x, y))\n",
        "\n",
        "            if player == 1 and y == size - 1:  # Player 1 goal: reach right edge\n",
        "                return True\n",
        "            if player == -1 and x == size - 1:  # Player -1 goal: reach bottom edge\n",
        "                return True\n",
        "\n",
        "            directions = [(-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0)]\n",
        "            for dx, dy in directions:\n",
        "                nx, ny = x + dx, y + dy\n",
        "                if 0 <= nx < size and 0 <= ny < size:\n",
        "                    if self.board[nx][ny] == player:\n",
        "                        if dfs(nx, ny):\n",
        "                            return True\n",
        "            return False\n",
        "\n",
        "        if player == 1:\n",
        "            for row in range(size):\n",
        "                if self.board[row][0] == player:\n",
        "                    if dfs(row, 0):\n",
        "                        return True\n",
        "        else:\n",
        "            for col in range(size):\n",
        "                if self.board[0][col] == player:\n",
        "                    if dfs(0, col):\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oec4oi1gwlgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Board Size so can train a bunch for different scenarios"
      ],
      "metadata": {
        "id": "-oFwosRGzTWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "import os\n",
        "\n",
        "def train_self_play_bot(size=5, total_checkpoints=5, timesteps_per_checkpoint=100_000):\n",
        "    print(f\"\\nTraining self-play bot on {size}x{size} board\")\n",
        "\n",
        "    # Set up environment\n",
        "    env = SelfPlayHexEnv(size=size)\n",
        "\n",
        "    # Create directory and naming convention\n",
        "    save_dir = f\"selfplay_hex_{size}x{size}\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Try to load existing model\n",
        "    latest_checkpoint = total_checkpoints * timesteps_per_checkpoint\n",
        "    latest_path = f\"{save_dir}/hex_bot_{latest_checkpoint}.zip\"\n",
        "\n",
        "    model_path = f\"{save_dir}/hex_bot_0.zip\"\n",
        "    if os.path.exists(latest_path):\n",
        "        model = PPO.load(latest_path, env=env)\n",
        "        print(f\"Resumed from: {latest_path}\")\n",
        "    elif os.path.exists(model_path):\n",
        "        model = PPO.load(model_path, env=env)\n",
        "        print(f\"Resumed from: {model_path}\")\n",
        "    else:\n",
        "        model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "        print(\"Starting new model.\")\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(1, total_checkpoints + 1):\n",
        "        total_steps = i * timesteps_per_checkpoint\n",
        "        print(f\"\\nCheckpoint {i}: {total_steps} steps\")\n",
        "        model.learn(total_timesteps=timesteps_per_checkpoint)\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = f\"{save_dir}/hex_bot_{total_steps}.zip\"\n",
        "        model.save(checkpoint_path)\n",
        "        print(f\"Saved to {checkpoint_path}\")\n"
      ],
      "metadata": {
        "id": "I9ozuKSQxb8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_self_play_bot(size=6, total_checkpoints=3)  # 300k steps on 6x6\n",
        "# train_self_play_bot(size=7, total_checkpoints=3)  # 300k steps"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yKhHoqu3xoS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gc2LiIlWzk6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOURNAMENT DAY GAMEPLAY SETUP**"
      ],
      "metadata": {
        "id": "llRi2DDM1Bzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Libraries needed\n",
        "from stable_baselines3 import PPO\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "YIu-P3jy1ETX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SETUP ===\n",
        "board_size = 6\n",
        "goes_first = False\n",
        "\n",
        "your_player = 1 if goes_first else -1\n",
        "opponent_player = -your_player"
      ],
      "metadata": {
        "id": "kOKnmi6K1J6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# env and model\n",
        "model = PPO.load(f\"selfplay_hex_{board_size}x{board_size}/hex_bot_300000.zip\")\n",
        "\n",
        "env = SelfPlayHexEnv(size=board_size)\n",
        "env.board = np.zeros((board_size, board_size), dtype=np.int8)\n",
        "env.current_player = 1  # Hex always starts with Player 1"
      ],
      "metadata": {
        "id": "QMjeGGAF1ROj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_board(board):\n",
        "    symbols = {0: \".\", 1: \"X\", -1: \"O\"}\n",
        "    for row in board:\n",
        "        print(\" \".join(symbols[int(cell)] for cell in row))\n",
        "    print()"
      ],
      "metadata": {
        "id": "mSoq1QK31TKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === GAME ON!!! ===\n",
        "print(\"Starting Hex Match!\")\n",
        "print(f\"You are playing as {'Player 1 (X)' if your_player == 1 else 'Player 2 (O)'}\")\n",
        "print_board(env.board)\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    # === Opponent's Turn (manually input this) ===\n",
        "    if env.current_player == opponent_player:\n",
        "        while True:\n",
        "            try:\n",
        "                x = int(input(\"Opponent move - Row: \"))\n",
        "                y = int(input(\"Opponent move - Column: \"))\n",
        "                if env.board[x][y] == 0:\n",
        "                    env.board[x][y] = opponent_player\n",
        "                    env.current_player = your_player\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Cell already occupied. Try again.\")\n",
        "            except:\n",
        "                print(\"Invalid input. Try again.\")\n",
        "        print(\"Opponent moved:\")\n",
        "        print_board(env.board)\n",
        "\n",
        "    # === My SuperBot's Turn ===\n",
        "    else:\n",
        "        import torch\n",
        "        obs = env.board.flatten().astype(np.float32)\n",
        "        legal_moves = [(i, j) for i in range(board_size) for j in range(board_size) if env.board[i][j] == 0]\n",
        "        legal_indices = [i * board_size + j for i, j in legal_moves]\n",
        "\n",
        "        dist = model.policy.get_distribution(torch.tensor(obs).unsqueeze(0))\n",
        "        probs = dist.distribution.probs.detach().numpy().flatten()\n",
        "\n",
        "        masked_probs = np.zeros_like(probs)\n",
        "        masked_probs[legal_indices] = probs[legal_indices]\n",
        "        masked_probs /= masked_probs.sum()\n",
        "\n",
        "        action = np.random.choice(len(masked_probs), p=masked_probs)\n",
        "        x, y = divmod(action, board_size)\n",
        "        env.board[x][y] = your_player\n",
        "        env.current_player = opponent_player\n",
        "\n",
        "        print(f\"Bot moves: ({x}, {y})\")\n",
        "        print_board(env.board)\n",
        "\n",
        "    # === Check Win ===\n",
        "    if env.check_win():\n",
        "        winner = \"Bot (You)\" if env.current_player == opponent_player else \"Opponent\"\n",
        "        print(f\"Game Over! Winner: {winner}\")\n",
        "        done = True"
      ],
      "metadata": {
        "id": "br26AQd31Uxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remember to save before exiting if you are in Colab"
      ],
      "metadata": {
        "id": "lqOozbAJ7NMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"selfplay_hex_6x6/hex_bot_300000.zip\")"
      ],
      "metadata": {
        "id": "6N6GyexG1zwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "01AD6ikn7dJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}